# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vMcbKLK8Trnsa1RaSEhVKEo8N0QPsk3A
"""

import streamlit as st
import pickle
import numpy as np

# Load the trained model
with open('model.pkl', 'rb') as file:
    model = pickle.load(file)

with open("ss_features.pkl","rb") as file:
  ss_features=pickle.load(file)





# App title
st.title("Classification Model Deployment")

# Input fields (update according to your feature names)
st.header("Enter Features")
# Add more inputs as needed
google_index = st.number_input("google_index", min_value=0, max_value=4.0)
nb_hyperlinks = st.number_input("nb_hyperlinks", min_value=0, max_value=4.0)
page_rank = st.number_input("page_rank", min_value=0, max_value=4.0)
web_traffic = st.number_input("web_traffic", min_value=0, max_value=4.0)
domain_age = st.number_input("domain_age", min_value=0, max_value=4.0)
nb_www = st.number_input("nb_www", min_value=0, max_value=4.0)
ratio_intHyperlinks = st.number_input("ratio_intHyperlinks", min_value=0, max_value=4.0)
ratio_extHyperlinks = st.number_input("ratio_extHyperlinks", min_value=0, max_value=4.0)
# Predict
if st.button("Predict"):
    input_data = np.array([['google_index','nb_hyperlinks','page_rank','web_traffic','domain_age,nb_www','ratio_intHyperlinks','ratio_extHyperlinks']])  # Update based on number of features
    prediction = model.predict(input_data)
    st.success(f"Predicted Class: {prediction[0]}")

